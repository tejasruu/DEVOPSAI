# -*- coding: utf-8 -*-
"""CREDIT RISK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LtIfB5UMPErGWouV9o_cGfEkO7dtwjLf
"""

!pip install -U imbalanced-learn

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
file_path = '/content/credit_risk_dataset.csv'  # Update with your file path
df = pd.read_csv(file_path)

# Step 1: Handling missing values
df['person_emp_length'].fillna(df['person_emp_length'].median(), inplace=True)
df['loan_int_rate'].fillna(df['loan_int_rate'].mean(), inplace=True)

# Step 2: Encoding categorical variables
label_encoders = {}
for column in ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']:
    label_encoders[column] = LabelEncoder()
    df[column] = label_encoders[column].fit_transform(df[column])

# Step 3: Splitting data into features (X) and target (y)
X = df.drop(columns=['loan_status'])
y = df['loan_status']

# Step 4: Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Step 5: Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)

# Step 6: Scaling the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 7: Setting up XGBoost classifier
xgb_model = xgb.XGBClassifier(random_state=42)

# Step 8: Hyperparameter tuning with GridSearchCV
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300],
    'colsample_bytree': [0.3, 0.7]
}

grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1)
grid_search.fit(X_train, y_train)

# Step 9: Best model from GridSearchCV
best_model = grid_search.best_estimator_

# Step 10: Predicting on test data
y_pred = best_model.predict(X_test)

# Step 11: Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Classification Report:\n{classification_rep}")

# Step 12: Plotting confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.show()

# Step 13: Function to take user input and predict future risk
def predict_risk():
    print("\nPlease enter the following details to predict loan status:")
    person_age = int(input("Person's Age: "))
    person_income = float(input("Person's Income: "))
    person_home_ownership = input("Home Ownership (0: RENT, 1: MORTGAGE, 2: OWN): ")
    person_emp_length = float(input("Employment Length (in years): "))
    loan_intent = input("Loan Intent (0: PERSONAL, 1: EDUCATION, etc.): ")
    loan_grade = input("Loan Grade (0-6): ")
    loan_amnt = float(input("Loan Amount: "))
    loan_int_rate = float(input("Loan Interest Rate: "))
    cb_person_default_on_file = input("Default on file (0: No, 1: Yes): ")
    cb_person_cred_hist_length = float(input("Credit History Length: "))

    try:
        # Validate each categorical input
        if person_home_ownership not in label_encoders['person_home_ownership'].classes_:
            raise ValueError("Invalid Home Ownership value. Please enter 0, 1, or 2.")
        person_home_ownership = label_encoders['person_home_ownership'].transform([person_home_ownership])[0]

        if loan_intent not in label_encoders['loan_intent'].classes_:
            raise ValueError("Invalid Loan Intent value. Please use valid options like 0, 1, etc.")
        loan_intent = label_encoders['loan_intent'].transform([loan_intent])[0]

        if loan_grade not in label_encoders['loan_grade'].classes_:
            raise ValueError("Invalid Loan Grade. Please enter a grade from 0 to 6.")
        loan_grade = label_encoders['loan_grade'].transform([loan_grade])[0]

        if cb_person_default_on_file not in label_encoders['cb_person_default_on_file'].classes_:
            raise ValueError("Invalid Default on File status. Please enter 0 or 1.")
        cb_person_default_on_file = label_encoders['cb_person_default_on_file'].transform([cb_person_default_on_file])[0]

        # Prepare input for prediction
        user_data = np.array([[person_age, person_income, person_home_ownership, person_emp_length,
                               loan_intent, loan_grade, loan_amnt, loan_int_rate,
                               cb_person_default_on_file, cb_person_cred_hist_length]])

        # Scale input
        user_data = scaler.transform(user_data)

        # Predict risk
        prediction = best_model.predict(user_data)
        prediction_prob = best_model.predict_proba(user_data)

        # Output result
        risk_status = "Approved" if prediction[0] == 1 else "Not Approved"
        print(f"\nPrediction: Loan Status is {risk_status}")
        print(f"Probability of Approval: {prediction_prob[0][1]:.2f}")
        print(f"Probability of Rejection: {prediction_prob[0][0]:.2f}")

    except ValueError as ve:
        print(f"Input Error: {ve}")


# Step 14: Feature Importance Plot
def plot_feature_importance():
    # Get feature importance from the best model
    importance = best_model.feature_importances_

    # Get feature names
    features = X.columns

    # Create a bar chart
    plt.figure(figsize=(10, 6))
    sns.barplot(x=importance, y=features)
    plt.title("Feature Importance in Loan Risk Prediction")
    plt.xlabel("Importance Score")
    plt.ylabel("Feature")
    plt.show()

# Call functions
predict_risk()  # Take user input and predict risk
plot_feature_importance()  # Plot feature importance
